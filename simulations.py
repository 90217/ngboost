import numpy as np
import pandas as pd
import scipy as sp
import scipy.stats
from survboost import SurvBoost
from scoring_rules import *
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from torch.distributions.log_normal import LogNormal, Normal
from evaluation import *
from scipy.stats import norm

'''
Below are a set of functions for parameters
'''
def f_const(X, const):
    '''
    A contant function of X. 
    '''
    return [const] * len(X)

def f_linear(X, coef):
    '''
    A linear function of X.
    coef :: a list of coefficients for covariates of X
    '''
    return np.sum(coef * X, axis=1)

def f_linear_exp(X, coef):
    '''
    A linear exponential function of X. Note that might need to adjust expectation.
    coef :: a list of coefficients for covariates of X
    '''
    return np.sum(coef ** X, axis=1)

def f_custom(X):
    '''
    A non-linear non-monotonic fucntion of X.
    '''
    pnorm = norm.cdf
    res = 4 * (X[:,0] > 1) * (X[:,1] > 0) + 4 * (X[:,2] > 1) * (X[:,3] > 0) + \
            2 * X[:,4] * X[:,0] - 4 * pnorm(-1) #adjust expectation
    return res

'''
Simulation according to the proposal 
'''
def simulate_data_v1(N=50, M=5, cov_x=0.5, D = sp.stats.genextreme,
                        D_config={'scale':1, 'shape':0}):

    '''
    Generate simulated data (unnoised version).
    
    Input:
    N :: number of data points generated
    M :: number of covariates in X
    cov_x :: covariance of Xs 
    D :: conditional outcome distribtuion, can choose from sp.stats.genextreme, sp.stats.lognorm, and etc
    D_config :: parameters of the distribution, each can be generated by a customized function,
                such as f_const, f_linear, f_linear_exp, f_custom
    
    Returns: (Y, X)
    '''
    
    cov_matrix = np.ones((M, M)) * cov_x + np.eye(M) * (1 - cov_x)
    X = sp.stats.multivariate_normal.rvs(cov=cov_matrix, size=N)
    D_shape = abs(f_custom(X))
    D_scale = f_const(X, 1.5)
    D_config['shape'] = D_shape
    D_config['scale'] = D_scale
    Y = D.var(c=D_config['shape'], scale=D_config['scale'])
    return (Y, X)

'''
Simulation according to [1]; calculate a fixed sigma based on outcome and noise distributions

[1] Schmid, Matthias, and Torsten Hothorn. Flexible Boosting of Accelerated Failure Time Models.
https://doi.org/10.1186/1471-2105-9-269.
'''

def _var_w(noise_dist, noise_config):
    '''
    Helper to calcualte the variance of the selected distribution 
    '''
    if noise_dist == sp.stats.genextreme:
        return noise_dist.var(c=noise_config['c'], scale=noise_config['scale'])
    else:
        return noise_dist.var(loc=noise_config['loc'], scale=noise_config['scale'])


def _sample_w(noise_dist, noise_config, N=50):
    '''
    Sample from a selected distribution
    '''
    if noise_dist == sp.stats.genextreme:
        return noise_dist.rvs(c=noise_config['c'], scale=noise_config['scale'])
    else:
        return noise_dist.rvs(loc=noise_config['loc'], scale=noise_config['scale'], size=N)


def simulate_data_v2(N=50, M=5, beta=[0.5, 0.25, -0.25, -0.5, 0.5], cov_x=0.5, var_explained=0.8, \
                        noise_dist=sp.stats.norm, noise_config={'loc':0, 'scale':1}):

    '''
    Generate simulated data (with specified noise).
    Note that this for now only supports linear models f(X) for log(T) with noise variable W.
    
    Input:
    N :: number of data points generated
    M :: number of covariates
    beta :: a list of coefficients for the covariates
    cov_x :: covariance of Xs 
    var_explained :: variance of log(T) explianed by f(X)
    nosie_dist :: choose from [sp.stats.norm, sp.stats.genextreme, sp.stats.logistic]
    noise_config :: configurations of the distribution

    Returns: (Y, X, beta)
    '''

    cov_matrix = np.ones((M, M)) * cov_x + np.eye(M) * (1 - cov_x)
    beta = np.array(beta)[np.newaxis]
    
    # calculate a fixed sigma before simulation: var(sum(beta*x)) = sum(var(beta*x)) + 2sum(cov(beta_i*x_i, beta_j*x_j))
    var_linear = np.sum(beta**2) + cov_x * (np.sum(beta.T*beta) - np.sum(beta**2))
    var_w = _var_w(noise_dist, noise_config)
    sigma = np.sqrt((1. - var_explained) * var_linear / (var_explained * var_w))
    
    covariates = sp.stats.multivariate_normal.rvs(cov=cov_matrix, size=N)
    unnoised = np.dot(covariates, beta.flatten())
    noisy = unnoised + sigma * _sample_w(noise_dist, noise_config, N)
    T = np.exp(noisy)
    return (T, covariates, beta)
    

def gen_sim_data(N=50, M=5, var_explained=0.8, noise_dist=sp.stats.norm):
    """
    Generate simulated data, according to the process described in [1].
    
    Noise distribution should be one of:
    [ sp.stats.norm, sp.stats.genextreme, sp.stats.logistic ]
    
    Returns: (Y, X, beta)
    
    [1] Schmid, Matthias, and Torsten Hothorn. 
    Flexible Boosting of Accelerated Failure Time Models.
    BMC Bioinformatics 9 (June 6, 2008): 269. 
    https://doi.org/10.1186/1471-2105-9-269.
    """
    cov_matrix = np.ones((M, M)) * 0.5 + np.eye(M) * 0.5
    covariates = sp.stats.multivariate_normal.rvs(cov=cov_matrix, size=N)
    beta = np.r_[np.array((0.5, 0.25, -0.25, -0.5, 0.5)), np.zeros(M - 5)]
    unnoised = np.dot(covariates, beta)
    sigma = np.sqrt(np.var(unnoised) * (1 / var_explained - 1))
    noisy = unnoised + sigma * noise_dist.rvs(size=N)
    times = np.exp(noisy)
    return (times, covariates, beta)
    

def eval_preds(filename):
    preds = np.load(filename)
    df = pd.read_csv("data/simulated/sim_data_test.csv")
    print("Concordance: %.4f" % calculate_concordance_naive(preds, df["Y"], df["C"]))
    print('Pred_mean: %.4f, True_mean: %.2f' % (np.mean(preds), np.mean(df["Y"])))
    
    
def run_survboost():

    df_train = pd.read_csv("data/simulated/sim_data_train.csv")
    df_test = pd.read_csv("data/simulated/sim_data_test.csv")

    sb = SurvBoost(Base = lambda: DecisionTreeRegressor(criterion='mse'),
                   Dist = LogNormal,
                   Score = CRPS_surv,
                   n_estimators = 200)

    sb.fit(df_train.drop(["Y", "C"], axis=1).as_matrix(), 
           df_train["Y"], df_train["C"])
    preds_test = sb.pred_mean(df_test.drop(["Y", "C"], axis=1))
    np.save("data/simulated/sim_preds_survboost.npy", preds_test)

    
if __name__ == "__main__":
    
    Y, X, beta = gen_sim_data(N=1000, var_explained = 0.8)
    #Y, X = simulate_data_v1(N=1000)
    CENSORED_FRAC = 0.5
    C = np.zeros_like(Y)
    C[:int(CENSORED_FRAC * len(Y))] = 1
    df = pd.DataFrame(X, columns=["X%d" % i for i in range(X.shape[1])])
    df["Y"] = Y
    df["C"] = C
    df = df.sample(frac=1, replace=False)
    df.iloc[:500].to_csv("data/simulated/sim_data_train.csv", index=False)
    df.iloc[500:].to_csv("data/simulated/sim_data_test.csv", index=False)

    
